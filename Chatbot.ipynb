{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e940a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m100.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/rohan/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m108.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m73.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03abce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rohan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import json,urllib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6472cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b0c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    \"\"\"\n",
    "    sentence = [\"hello, \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag =   [0,     1,       0,   1,    0,      0,       0 ]\n",
    "    \"\"\"\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(all_words), dtype = np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b1887d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>dialogueID</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>1.tsv</td>\n",
       "      <td>2004-11-23T11:49:00.000Z</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>any ideas why java plugin takes so long to load?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>1.tsv</td>\n",
       "      <td>2004-11-23T11:49:00.000Z</td>\n",
       "      <td>crimsun</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>java 1.4?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>301</td>\n",
       "      <td>1.tsv</td>\n",
       "      <td>2004-11-23T11:49:00.000Z</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>crimsun</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301</td>\n",
       "      <td>1.tsv</td>\n",
       "      <td>2004-11-23T11:49:00.000Z</td>\n",
       "      <td>crimsun</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>java 1.5 loads _much_ faster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>301</td>\n",
       "      <td>1.tsv</td>\n",
       "      <td>2004-11-23T11:50:00.000Z</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>crimsun</td>\n",
       "      <td>noneus: how can i get 1.5 is there a .deb some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   folder dialogueID                      date     from       to  \\\n",
       "0     301      1.tsv  2004-11-23T11:49:00.000Z  stuNNed      NaN   \n",
       "1     301      1.tsv  2004-11-23T11:49:00.000Z  crimsun  stuNNed   \n",
       "2     301      1.tsv  2004-11-23T11:49:00.000Z  stuNNed  crimsun   \n",
       "3     301      1.tsv  2004-11-23T11:49:00.000Z  crimsun  stuNNed   \n",
       "4     301      1.tsv  2004-11-23T11:50:00.000Z  stuNNed  crimsun   \n",
       "\n",
       "                                                text  \n",
       "0   any ideas why java plugin takes so long to load?  \n",
       "1                                          java 1.4?  \n",
       "2                                                yes  \n",
       "3                       java 1.5 loads _much_ faster  \n",
       "4  noneus: how can i get 1.5 is there a .deb some...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Three files : \n",
    "# 1.ubuntu_forum_data/Ubuntu-dialogue-corpus/dialogueText.csv -> 1038325 datapoints (116MB)\n",
    "# 2.ubuntu_forum_data/Ubuntu-dialogue-corpus/dialogueText_196.csv -> 1048576 datapoints (996MB)\n",
    "# 3.ubuntu_forum_data/Ubuntu-dialogue-corpus/dialogueText_301.csv -> 1048576 datapoints (1.8GB)\n",
    "\n",
    "df = pd.read_csv(\"/media/rohan/DATA/AI_fellowship_2022/Course_2_DL/Ubuntu_chatbot/ubuntu_forum_data/Ubuntu-dialogue-corpus/dialogueText_301.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a01c28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuNNed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>any ideas why java plugin takes so long to load?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crimsun</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>java 1.4?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stuNNed</td>\n",
       "      <td>crimsun</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crimsun</td>\n",
       "      <td>stuNNed</td>\n",
       "      <td>java 1.5 loads _much_ faster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stuNNed</td>\n",
       "      <td>crimsun</td>\n",
       "      <td>noneus: how can i get 1.5 is there a .deb some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      from       to                                               text\n",
       "0  stuNNed      NaN   any ideas why java plugin takes so long to load?\n",
       "1  crimsun  stuNNed                                          java 1.4?\n",
       "2  stuNNed  crimsun                                                yes\n",
       "3  crimsun  stuNNed                       java 1.5 loads _much_ faster\n",
       "4  stuNNed  crimsun  noneus: how can i get 1.5 is there a .deb some..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = df[['from','to','text']]\n",
    "conversation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5c5375d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23215/895046125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(convo[\"text\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconvo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "#extract all words from the ubuntu form dataset\n",
    "all_words = []\n",
    "x = []\n",
    "for index, convo in conversation.iterrows():\n",
    "    # print(convo[\"text\"])\n",
    "    for pattern in convo[\"text\"] :\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        x.append(w)\n",
    "ignore_words = ['?','!','.',',']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = []\n",
    "y_train = []\n",
    "for pattern_sentence in x:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "y_train = X_train[1:]\n",
    "X_train = X_train[:-1]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    # dataset[idx]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Hyper paramters\n",
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(y_train[0])\n",
    "input_size = len(X_train[0])\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset = dataset, batch_size = batch_size,\n",
    "                        shuffle =False, num_workers=2)\n",
    "\n",
    "device  = torch.device('cpu')\n",
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch +1) % 100 == 0:\n",
    "        # f'epoch {epoch+1}/{num_epochs}, loss={loss.item():.4f}'\n",
    "        print(\"epoch {}/{}, loss={:.4f}.\".format(epoch+1,num_epochs,loss.item()))\n",
    "\n",
    "# print(f'Final loss, loss={loss.item():.4f}')\n",
    "print(\"Final Loss, loss{:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model_state\":model.state_dict(),\n",
    "    \"input_size\":input_size,\n",
    "    \"output_size\":output_size,\n",
    "    \"hiddent_size\": hidden_size,\n",
    "    \"all_words\":all_words\n",
    "}\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(\"Traning complete. file saved to\",FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device  = torch.device('cpu')\n",
    "# print(device)\n",
    "\n",
    "# with open(url,'r') as f:\n",
    "#   intents = json.load(f) \n",
    "\n",
    "\n",
    "# FILE = \"data.pth\"\n",
    "# data = torch.load(FILE)\n",
    "\n",
    "# input_size = data['input_size']\n",
    "# hidden_size = data['hiddent_size']\n",
    "# output_size = data['output_size']\n",
    "# all_words = data['all_words']\n",
    "# tags = data['tags']\n",
    "# model_state = data['model_state']\n",
    "\n",
    "# model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "# model.load_state_dict(model_state)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc6a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callme():\n",
    "  sen = input(\"Ubuntu_bro : Enter a query \\n You: \")\n",
    "  return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3b7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          folder dialogueID                      date       from       to  \\\n",
      "0            301      1.tsv  2004-11-23T11:49:00.000Z    stuNNed      NaN   \n",
      "1            301      1.tsv  2004-11-23T11:49:00.000Z    crimsun  stuNNed   \n",
      "2            301      1.tsv  2004-11-23T11:49:00.000Z    stuNNed  crimsun   \n",
      "3            301      1.tsv  2004-11-23T11:49:00.000Z    crimsun  stuNNed   \n",
      "4            301      1.tsv  2004-11-23T11:50:00.000Z    stuNNed  crimsun   \n",
      "...          ...        ...                       ...        ...      ...   \n",
      "16587825      32   1783.tsv  2007-11-15T03:38:00.000Z    koyo001      NaN   \n",
      "16587826      32   1783.tsv  2007-11-15T03:39:00.000Z    koyo001      NaN   \n",
      "16587827      32   1783.tsv  2007-11-15T03:39:00.000Z  neverblue      NaN   \n",
      "16587828      32   1783.tsv  2007-11-15T03:40:00.000Z    koyo001   ikonia   \n",
      "16587829      32   1783.tsv  2007-11-15T03:40:00.000Z  neverblue  koyo001   \n",
      "\n",
      "                                                       text  \n",
      "0          any ideas why java plugin takes so long to load?  \n",
      "1                                                 java 1.4?  \n",
      "2                                                       yes  \n",
      "3                              java 1.5 loads _much_ faster  \n",
      "4         noneus: how can i get 1.5 is there a .deb some...  \n",
      "...                                                     ...  \n",
      "16587825                                             thanks  \n",
      "16587826                         does anyone know something  \n",
      "16587827                        no, no one knows everything  \n",
      "16587828                             the camera doesnt work  \n",
      "16587829  I believe you missed a post or two while you w...  \n",
      "\n",
      "[16587830 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "\n",
    "bot_name = \"Ubuntu_bro\"\n",
    "\n",
    "print(\"Namste !!! I am Ubuntu_bro. Just drop your Ubuntu related query \")\n",
    "while True:\n",
    "  sentence = input('You: ')\n",
    "\n",
    "  if sentence =='quit':\n",
    "    break\n",
    "  \n",
    "  sentence = tokenize(sentence)\n",
    "  X = bag_of_words(sentence, all_words)\n",
    "  X = X.reshape(1, X.shape[0])\n",
    "  X = torch.from_numpy(X)\n",
    "\n",
    "  output = model(X)\n",
    "  _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "  probs = torch.softmax(output, dim=1)\n",
    "  prob = probs[0][predicted.item()]\n",
    "\n",
    "# \n",
    "# \n",
    "\n",
    "  print(\"{} I do not understand...\".format(bot_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
