{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import json,urllib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    \"\"\"\n",
    "    sentence = [\"hello, \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag =   [0,     1,       0,   1,    0,      0,       0 ]\n",
    "    \"\"\"\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(all_words), dtype = np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6019788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset source\n",
    "# url = \"./ubuntu_forum_data/Ubuntu_dialogue_corpus/dialogueText.csv\"\n",
    "# intents = pd.read_csv(url)\n",
    "url = '/kaggle/input/chatbot-dataset/intents.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url, 'r') as f:\n",
    "    intents = json.load(f)\n",
    "    \n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))\n",
    "\n",
    "ignore_words = ['?','!','.',',']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags  = sorted(set(tags))\n",
    "\n",
    "X_train  = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    \n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label) # CrossEntropyLoss\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    # dataset[idx]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Hyper paramters\n",
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "input_size = len(X_train[0])\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset = dataset, batch_size = batch_size,\n",
    "                        shuffle =True, num_workers=2)\n",
    "\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch +1) % 100 == 0:\n",
    "        # f'epoch {epoch+1}/{num_epochs}, loss={loss.item():.4f}'\n",
    "        print(\"epoch {}/{}, loss={:.4f}.\".format(epoch+1,num_epochs,loss.item()))\n",
    "\n",
    "# print(f'Final loss, loss={loss.item():.4f}')\n",
    "print(\"Final Loss, loss{:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a742343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model_state\":model.state_dict(),\n",
    "    \"input_size\":input_size,\n",
    "    \"output_size\":output_size,\n",
    "    \"hiddent_size\": hidden_size,\n",
    "    \"all_words\":all_words,\n",
    "    \"tags\": tags\n",
    "}\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(\"Traning complete. file saved to\",FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "with open(url,'r') as f:\n",
    "  intents = json.load(f) \n",
    "\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data['input_size']\n",
    "hidden_size = data['hiddent_size']\n",
    "output_size = data['output_size']\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data['model_state']\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1887d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/all-india-pincode-directory-with-contact-details/all_india_PO_list_without_APS_offices_ver2_lat_long.csv\")\n",
    "df = df[['officename','pincode']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callme():\n",
    "  po = {v: k for v, k in enumerate(pf)}\n",
    "  print(\"Taru: Select your near Post Office \\n \")\n",
    "\n",
    "  for i,j in po.items():\n",
    "    print(i,j)\n",
    "\n",
    "  sel = int(input(\"\\n Enter Number\"))\n",
    "  print(\"\\n Taru: You selected {} Post office. \\n\".format(po[sel]))\n",
    "\n",
    "  sen = input(\"Taru: Enter another query \\n You: \")\n",
    "  return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd768a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_name = \"Taru\"\n",
    "\n",
    "print(\"Namste !!! We are working everywhere in Pan-india. Just drop you delivery location pincode/postal code to check delivery availability or ask us at Whatsapp Helpline 1234567890.\")\n",
    "while True:\n",
    "  sentence = input('You: ')\n",
    "\n",
    "  for word in sentence.split():\n",
    "    if word.isdigit():\n",
    "      word = int(word)\n",
    "      pf = list(df['Office Name'][df['Pincode'] == word ])\n",
    "      if len(pf) == 0:\n",
    "        print(\"Sorry to inform you, We don't deliver here :(\")\n",
    "        break\n",
    "      sentence = callme()\n",
    "      # sentence = input('You: ')\n",
    "    \n",
    "    # else:\n",
    "      # print(\"Please Enter Pincode !!!\")\n",
    "\n",
    "  if sentence =='quit':\n",
    "    break\n",
    "  \n",
    "  sentence = tokenize(sentence)\n",
    "  X = bag_of_words(sentence, all_words)\n",
    "  X = X.reshape(1, X.shape[0])\n",
    "  X = torch.from_numpy(X)\n",
    "\n",
    "  output = model(X)\n",
    "  _, predicted = torch.max(output, dim=1)\n",
    "  tag = tags[predicted.item()]\n",
    "\n",
    "  probs = torch.softmax(output, dim=1)\n",
    "  prob = probs[0][predicted.item()]\n",
    "\n",
    "\n",
    "  if prob.item() >0.75:\n",
    "    for intent in intents[\"intents\"]:\n",
    "      if tag == intent[\"tag\"]:\n",
    "        print(bot_name, \":\",random.choice(intent['responses']))\n",
    "        # print(\"{}:{}\".format(bot_name,random.choice(intent['responses'])))\n",
    "      \n",
    "  else:\n",
    "    print(\"{} I do not understand...contact on WhatsApp 1234567890\".format(bot_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
